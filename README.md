# Layerâ€‘wise Adaptive Sine ActivationÂ (LASA)

LASA replaces the usual sine activation with **one trainable frequency scalarÂ Ï„ per layer**:

``sin(Â Ï„â‚—Â Â·Â (wáµ€xÂ +Â b)Â )``

That single scalar lets each layer glide from an *almostâ€‘linear* regime (smallÂ |Ï„|) to highly oscillatory behaviour (largeÂ |Ï„|) **without adding inferenceâ€‘time cost**â€”Ï„ can be absorbed into the weights after training.

### Why you might care

* **Singleâ€‘neuron expressivity** â€“ one LASA unit solves XOR and can draw multiple closed loops that monotone activations cannot.  
* **Stable optimisation** â€“ Ï„ behaves like a layerâ€‘wise Lipschitz constant, so gradients stay healthy even in deep nets.  
* **Topâ€‘tier toyâ€‘set accuracy** â€“ 95.6Â % mean accuracy on 10 geometric datasets (vs. 77.9Â % for nextâ€‘best Mish).  
* **Realâ€‘benchmark gains** â€“ +7Â pp on CIFARâ€‘10 over ReLU using the same CNN.

---

# `decision_boundaries/` 

The folder contains **`DecisionÂ Boundaries.ipynb`**, which reproduces the key visual experiments (FiguresÂ 9â€‘12, TableÂ 1) from the LASA paper.
![image](https://github.com/user-attachments/assets/472a9012-1426-4e28-96b8-bd826bfb4e81)
![image](https://github.com/user-attachments/assets/4447ba0d-7472-499d-90cb-22824bf1d76c)
![image](https://github.com/user-attachments/assets/8b0cdb2f-454f-4e06-bbfd-6bbcacbc0955)




| What it does                                                                                    | How itâ€™s implemented                              |
| :------------------------------------------------------------------------------------------------ | :------------------------------------------------ |
| Generates 10 classic toy datasets (rings â†’ Lissajous)                                            | Pure NumPy utility functions                      |
| Trains 8 activation variants (LASA, ReLU, LeakyÂ ReLU, ELU, Softplus, Swish, Mish, GELU)          | Same 1â€‘hiddenâ€‘layer Keras MLP, AdamÂ 1eâ€‘3, 40Â epochs |
| Renders sideâ€‘byâ€‘side decisionâ€‘boundary heatâ€‘maps with test accuracy in the title                 | Matplotlib                                        |
| Exports a `results.csv` summary table (matches the paperâ€™s TableÂ 1)                              | Pandas DataFrame                                  |

### Key replicated results

| Dataset | Hidden units | LASAÂ acc. | Best other |
| ------- | ------------ | --------- | ---------- |
| Ring        | 2   | **1.00** | MishÂ 0.92 |
| Spiral      | 12  | **1.00** | SwishÂ 0.75 |
| Yinâ€‘Yang    | 16  | **0.98** | ReLUÂ 0.85 |
| Pinwheel    | 2   | **0.80** | SoftplusÂ 0.54 |


---

## ðŸ”§ Weightâ€‘Initialisation Sweep (`weight_init.ipynb`)

| Goal | Stressâ€‘test LASA (and baselines) under different weightâ€‘initialisation schemes |
|------|------------------------------------------------------------------------------|
| Schemes compared | **Xavier/Glorot**, **He/Kaiming**, **Orthogonal**, **UniformÂ Â±0.05**|
| Activations | LASAÂ (ours), Sine, ReLU, ELU, Mish |
| Metrics logged | Test accuracy, loss, gradientâ€‘norm statistics, final Ï„Â (frequency) values |

![image](https://github.com/user-attachments/assets/4ce0303e-96f9-4a3b-8c25-eea1c521498c)


### How to reproduce

```bash
# 1â€†Â·â€†Create / activate the environment
conda env create -f ../../env.yaml          # first time only
conda activate lasa

# 2â€†Â·â€†Run the notebook (will save results & figures automatically)
papermill weight_init.ipynb \
         -p dataset "mnist" \
         -p n_seeds 20


### Quickâ€‘start

```bash
conda env create -f environment.yml
conda activate lasa
jupyter notebook decision_boundaries/Decision\ Boundaries.ipynb

