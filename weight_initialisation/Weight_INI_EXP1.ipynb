{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce7b106b",
   "metadata": {},
   "source": [
    "## Weight initalisations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c486a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with xavier initializer...\n",
      "Initializer: xavier, Epoch 1/20, Avg Loss: 0.6152, Test Accuracy: 0.6950\n",
      "Initializer: xavier, Epoch 11/20, Avg Loss: 0.4794, Test Accuracy: 0.7900\n",
      "Initializer: xavier, Final Test Accuracy: 0.9875\n",
      "Initializer: xavier, Layer 1 - Initial k: 1.4, Final k: 0.4932\n",
      "Initializer: xavier, Layer 2 - Initial k: 1.6, Final k: 2.6732\n",
      "\n",
      "Running experiment with he initializer...\n",
      "Initializer: he, Epoch 1/20, Avg Loss: 0.6216, Test Accuracy: 0.7725\n",
      "Initializer: he, Epoch 11/20, Avg Loss: 0.3636, Test Accuracy: 0.9200\n",
      "Initializer: he, Final Test Accuracy: 0.9875\n",
      "Initializer: he, Layer 1 - Initial k: 1.4, Final k: 2.1664\n",
      "Initializer: he, Layer 2 - Initial k: 1.6, Final k: 3.0727\n",
      "\n",
      "Running experiment with lecun initializer...\n",
      "Initializer: lecun, Epoch 1/20, Avg Loss: 0.6180, Test Accuracy: 0.7450\n",
      "Initializer: lecun, Epoch 11/20, Avg Loss: 0.5442, Test Accuracy: 0.7450\n",
      "Initializer: lecun, Final Test Accuracy: 0.7450\n",
      "Initializer: lecun, Layer 1 - Initial k: 1.4, Final k: 0.5629\n",
      "Initializer: lecun, Layer 2 - Initial k: 1.6, Final k: 2.1298\n",
      "\n",
      "Running experiment with random initializer...\n",
      "Initializer: random, Epoch 1/20, Avg Loss: 0.7053, Test Accuracy: 0.5925\n",
      "Initializer: random, Epoch 11/20, Avg Loss: 0.2648, Test Accuracy: 0.9075\n",
      "Initializer: random, Final Test Accuracy: 0.9675\n",
      "Initializer: random, Layer 1 - Initial k: 1.4, Final k: 1.5422\n",
      "Initializer: random, Layer 2 - Initial k: 1.6, Final k: 2.0605\n",
      "\n",
      "Running experiment with uniform initializer...\n",
      "Initializer: uniform, Epoch 1/20, Avg Loss: 0.8241, Test Accuracy: 0.5325\n",
      "Initializer: uniform, Epoch 11/20, Avg Loss: 0.4167, Test Accuracy: 0.8875\n",
      "Initializer: uniform, Final Test Accuracy: 0.9825\n",
      "Initializer: uniform, Layer 1 - Initial k: 1.4, Final k: 1.3570\n",
      "Initializer: uniform, Layer 2 - Initial k: 1.6, Final k: 3.0255\n",
      "\n",
      "Summary of Results:\n",
      "xavier initializer: 0.9875\n",
      "he initializer: 0.9875\n",
      "lecun initializer: 0.7450\n",
      "random initializer: 0.9675\n",
      "uniform initializer: 0.9825\n",
      "\n",
      "Best initializer: xavier with accuracy 0.9875\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_circles\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Custom Sine Activation Layer\n",
    "class SineActivation(tf.keras.layers.Layer):\n",
    "    def __init__(self, initial_frequency, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.initial_frequency = initial_frequency\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.freq = self.add_weight(shape=(), \n",
    "                                    initializer=tf.keras.initializers.Constant(self.initial_frequency),\n",
    "                                    trainable=True, \n",
    "                                    name='freq')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.math.sin(self.freq * inputs)\n",
    "\n",
    "\n",
    "X, y = make_circles(n_samples=2000, noise=0.1, factor=0.5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def create_model(activation, hidden_units, k_values, initializer):\n",
    "    model = Sequential([layers.Input(shape=(2,))])\n",
    "    \n",
    "    sine_layers_k = []  \n",
    "    for i, units in enumerate(hidden_units):\n",
    "        if activation == 'sine':\n",
    "            dense_layer = layers.Dense(units, \n",
    "                                       kernel_initializer=initializer, \n",
    "                                       bias_initializer=initializer, \n",
    "                                       name=f'dense_{i}')\n",
    "            model.add(dense_layer)\n",
    "            sine_activation_layer = SineActivation(initial_frequency=k_values[i], name=f'sine_{i}')\n",
    "            model.add(sine_activation_layer)\n",
    "            sine_layers_k.append(sine_activation_layer)\n",
    "        else:\n",
    "            model.add(layers.Dense(units, \n",
    "                                   activation=activation, \n",
    "                                   kernel_initializer=initializer, \n",
    "                                   bias_initializer=initializer, \n",
    "                                   name=f'dense_{i}'))\n",
    "    \n",
    "    model.add(layers.Dense(1, \n",
    "                           activation='sigmoid', \n",
    "                           kernel_initializer=initializer, \n",
    "                           bias_initializer=initializer, \n",
    "                           name='output'))\n",
    "    return model, sine_layers_k\n",
    "\n",
    "\n",
    "def train_step(model, x_batch, y_batch, optimizer_main, optimizer_k):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(x_batch, training=True)\n",
    "        y_batch = tf.reshape(y_batch, (-1, 1))\n",
    "        loss = tf.keras.losses.binary_crossentropy(y_batch, predictions)\n",
    "        loss = tf.reduce_mean(loss)  # Ensure loss is a scalar\n",
    "    \n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    k_vars = [v for v in model.trainable_variables if 'freq' in v.name]\n",
    "    other_vars = [v for v in model.trainable_variables if 'freq' not in v.name]\n",
    "    \n",
    "    k_grads = [g for g, v in zip(grads, model.trainable_variables) if 'freq' in v.name]\n",
    "    other_grads = [g for g, v in zip(grads, model.trainable_variables) if 'freq' not in v.name]\n",
    "    \n",
    "    optimizer_k.apply_gradients(zip(k_grads, k_vars))\n",
    "    optimizer_main.apply_gradients(zip(other_grads, other_vars))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def run_experiment(initializer_name, initializer):\n",
    "    H = [4, 2]\n",
    "    K = [1.4, 1.6]  \n",
    "    learning_rate_main = 0.001\n",
    "    learning_rate_k = 0.01\n",
    "    epochs = 20\n",
    "    batch_size = 32\n",
    "\n",
    "    # Create model\n",
    "    model, sine_layers_k = create_model('sine', H, K, initializer)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # Create separate optimizers\n",
    "    optimizer_main = Adam(learning_rate=learning_rate_main)\n",
    "    optimizer_k = Adam(learning_rate=learning_rate_k)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            x_batch = X_train[i:i+batch_size]\n",
    "            y_batch = y_train[i:i+batch_size]\n",
    "            \n",
    "            batch_loss = train_step(model, x_batch, y_batch, optimizer_main, optimizer_k)\n",
    "            total_loss += batch_loss.numpy()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_loss = total_loss / num_batches\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "            print(f\"Initializer: {initializer_name}, Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Initializer: {initializer_name}, Final Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    num_params = model.count_params()\n",
    "    \n",
    "    save_results_to_csv(\n",
    "        test_accuracy=test_accuracy,\n",
    "        num_params=num_params,\n",
    "        activation='sine',\n",
    "        initializer=initializer_name,\n",
    "        dataset_name='Concentric Circles'\n",
    "    )\n",
    "    \n",
    "    for i, layer in enumerate(sine_layers_k):\n",
    "        print(f\"Initializer: {initializer_name}, Layer {i+1} - Initial k: {K[i]}, Final k: {layer.freq.numpy():.4f}\")\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "def save_results_to_csv(test_accuracy, num_params, activation, initializer, dataset_name):\n",
    "    csv_filename = 'experiment_results.csv'\n",
    "    headers = ['Test Accuracy', 'Number of Parameters', 'Activation', 'Initializer', 'Dataset']\n",
    "\n",
    "    row = [test_accuracy, num_params, activation, initializer, dataset_name]\n",
    "\n",
    "    file_exists = os.path.isfile(csv_filename)\n",
    "    \n",
    "    with open(csv_filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writerow(headers)\n",
    "        \n",
    "        writer.writerow(row)\n",
    "\n",
    "\n",
    "initializers = {\n",
    "    'xavier': tf.keras.initializers.GlorotUniform(),\n",
    "    'he': tf.keras.initializers.HeNormal(),\n",
    "    'lecun': tf.keras.initializers.LecunUniform(),\n",
    "    'random': tf.keras.initializers.RandomUniform(minval=-1.5, maxval=1.5),\n",
    "    'uniform': tf.keras.initializers.RandomUniform(minval=-1.5, maxval=1.5)\n",
    "}\n",
    "\n",
    "\n",
    "results = {}\n",
    "for name, initializer in initializers.items():\n",
    "    print(f\"\\nRunning experiment with {name} initializer...\")\n",
    "    accuracy = run_experiment(name, initializer)\n",
    "    results[name] = accuracy\n",
    "\n",
    "print(\"\\nSummary of Results:\")\n",
    "for name, accuracy in results.items():\n",
    "    print(f\"{name} initializer: {accuracy:.4f}\")\n",
    "\n",
    "best_initializer = max(results, key=results.get)\n",
    "print(f\"\\nBest initializer: {best_initializer} with accuracy {results[best_initializer]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c83785",
   "metadata": {},
   "source": [
    "## All Weight initialisation With activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0e0490d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running experiment with relu activation and xavier initializer...\n",
      "Activation: relu, Initializer: xavier, Final Test Accuracy: 0.5075, Num Params: 25\n",
      "\n",
      "Running experiment with relu activation and he initializer...\n",
      "Activation: relu, Initializer: he, Final Test Accuracy: 0.7700, Num Params: 25\n",
      "\n",
      "Running experiment with relu activation and lecun initializer...\n",
      "Activation: relu, Initializer: lecun, Final Test Accuracy: 0.6600, Num Params: 25\n",
      "\n",
      "Running experiment with relu activation and random initializer...\n",
      "Activation: relu, Initializer: random, Final Test Accuracy: 0.4925, Num Params: 25\n",
      "\n",
      "Running experiment with relu activation and uniform initializer...\n",
      "Activation: relu, Initializer: uniform, Final Test Accuracy: 0.7125, Num Params: 25\n",
      "\n",
      "Running experiment with leaky_relu activation and xavier initializer...\n",
      "Activation: leaky_relu, Initializer: xavier, Final Test Accuracy: 0.5975, Num Params: 25\n",
      "\n",
      "Running experiment with leaky_relu activation and he initializer...\n",
      "Activation: leaky_relu, Initializer: he, Final Test Accuracy: 0.6875, Num Params: 25\n",
      "\n",
      "Running experiment with leaky_relu activation and lecun initializer...\n",
      "Activation: leaky_relu, Initializer: lecun, Final Test Accuracy: 0.6050, Num Params: 25\n",
      "\n",
      "Running experiment with leaky_relu activation and random initializer...\n",
      "Activation: leaky_relu, Initializer: random, Final Test Accuracy: 0.6900, Num Params: 25\n",
      "\n",
      "Running experiment with leaky_relu activation and uniform initializer...\n",
      "Activation: leaky_relu, Initializer: uniform, Final Test Accuracy: 0.5825, Num Params: 25\n",
      "\n",
      "Running experiment with elu activation and xavier initializer...\n",
      "Activation: elu, Initializer: xavier, Final Test Accuracy: 0.5275, Num Params: 25\n",
      "\n",
      "Running experiment with elu activation and he initializer...\n",
      "Activation: elu, Initializer: he, Final Test Accuracy: 0.6750, Num Params: 25\n",
      "\n",
      "Running experiment with elu activation and lecun initializer...\n",
      "Activation: elu, Initializer: lecun, Final Test Accuracy: 0.6075, Num Params: 25\n",
      "\n",
      "Running experiment with elu activation and random initializer...\n",
      "Activation: elu, Initializer: random, Final Test Accuracy: 0.4925, Num Params: 25\n",
      "\n",
      "Running experiment with elu activation and uniform initializer...\n",
      "Activation: elu, Initializer: uniform, Final Test Accuracy: 0.4925, Num Params: 25\n",
      "\n",
      "Running experiment with softplus activation and xavier initializer...\n",
      "Activation: softplus, Initializer: xavier, Final Test Accuracy: 0.5750, Num Params: 25\n",
      "\n",
      "Running experiment with softplus activation and he initializer...\n",
      "Activation: softplus, Initializer: he, Final Test Accuracy: 0.4950, Num Params: 25\n",
      "\n",
      "Running experiment with softplus activation and lecun initializer...\n",
      "Activation: softplus, Initializer: lecun, Final Test Accuracy: 0.4975, Num Params: 25\n",
      "\n",
      "Running experiment with softplus activation and random initializer...\n",
      "Activation: softplus, Initializer: random, Final Test Accuracy: 0.4925, Num Params: 25\n",
      "\n",
      "Running experiment with softplus activation and uniform initializer...\n",
      "Activation: softplus, Initializer: uniform, Final Test Accuracy: 0.4925, Num Params: 25\n",
      "\n",
      "Running experiment with swish activation and xavier initializer...\n",
      "Activation: swish, Initializer: xavier, Final Test Accuracy: 0.5075, Num Params: 25\n",
      "\n",
      "Running experiment with swish activation and he initializer...\n",
      "Activation: swish, Initializer: he, Final Test Accuracy: 0.8075, Num Params: 25\n",
      "\n",
      "Running experiment with swish activation and lecun initializer...\n",
      "Activation: swish, Initializer: lecun, Final Test Accuracy: 0.6175, Num Params: 25\n",
      "\n",
      "Running experiment with swish activation and random initializer...\n",
      "Activation: swish, Initializer: random, Final Test Accuracy: 0.4925, Num Params: 25\n",
      "\n",
      "Running experiment with swish activation and uniform initializer...\n",
      "Activation: swish, Initializer: uniform, Final Test Accuracy: 0.4925, Num Params: 25\n",
      "\n",
      "Running experiment with mish activation and xavier initializer...\n",
      "Activation: mish, Initializer: xavier, Final Test Accuracy: 0.5075, Num Params: 25\n",
      "\n",
      "Running experiment with mish activation and he initializer...\n",
      "Activation: mish, Initializer: he, Final Test Accuracy: 0.8250, Num Params: 25\n",
      "\n",
      "Running experiment with mish activation and lecun initializer...\n",
      "Activation: mish, Initializer: lecun, Final Test Accuracy: 0.6300, Num Params: 25\n",
      "\n",
      "Running experiment with mish activation and random initializer...\n",
      "Activation: mish, Initializer: random, Final Test Accuracy: 0.4925, Num Params: 25\n",
      "\n",
      "Running experiment with mish activation and uniform initializer...\n",
      "Activation: mish, Initializer: uniform, Final Test Accuracy: 0.4925, Num Params: 25\n",
      "\n",
      "Running experiment with gelu activation and xavier initializer...\n",
      "Activation: gelu, Initializer: xavier, Final Test Accuracy: 0.5075, Num Params: 25\n",
      "\n",
      "Running experiment with gelu activation and he initializer...\n",
      "Activation: gelu, Initializer: he, Final Test Accuracy: 0.8375, Num Params: 25\n",
      "\n",
      "Running experiment with gelu activation and lecun initializer...\n",
      "Activation: gelu, Initializer: lecun, Final Test Accuracy: 0.6250, Num Params: 25\n",
      "\n",
      "Running experiment with gelu activation and random initializer...\n",
      "Activation: gelu, Initializer: random, Final Test Accuracy: 0.5725, Num Params: 25\n",
      "\n",
      "Running experiment with gelu activation and uniform initializer...\n",
      "Activation: gelu, Initializer: uniform, Final Test Accuracy: 0.7525, Num Params: 25\n",
      "\n",
      "Summary of Results:\n",
      "relu activation with xavier initializer: 0.5075\n",
      "relu activation with he initializer: 0.7700\n",
      "relu activation with lecun initializer: 0.6600\n",
      "relu activation with random initializer: 0.4925\n",
      "relu activation with uniform initializer: 0.7125\n",
      "leaky_relu activation with xavier initializer: 0.5975\n",
      "leaky_relu activation with he initializer: 0.6875\n",
      "leaky_relu activation with lecun initializer: 0.6050\n",
      "leaky_relu activation with random initializer: 0.6900\n",
      "leaky_relu activation with uniform initializer: 0.5825\n",
      "elu activation with xavier initializer: 0.5275\n",
      "elu activation with he initializer: 0.6750\n",
      "elu activation with lecun initializer: 0.6075\n",
      "elu activation with random initializer: 0.4925\n",
      "elu activation with uniform initializer: 0.4925\n",
      "softplus activation with xavier initializer: 0.5750\n",
      "softplus activation with he initializer: 0.4950\n",
      "softplus activation with lecun initializer: 0.4975\n",
      "softplus activation with random initializer: 0.4925\n",
      "softplus activation with uniform initializer: 0.4925\n",
      "swish activation with xavier initializer: 0.5075\n",
      "swish activation with he initializer: 0.8075\n",
      "swish activation with lecun initializer: 0.6175\n",
      "swish activation with random initializer: 0.4925\n",
      "swish activation with uniform initializer: 0.4925\n",
      "mish activation with xavier initializer: 0.5075\n",
      "mish activation with he initializer: 0.8250\n",
      "mish activation with lecun initializer: 0.6300\n",
      "mish activation with random initializer: 0.4925\n",
      "mish activation with uniform initializer: 0.4925\n",
      "gelu activation with xavier initializer: 0.5075\n",
      "gelu activation with he initializer: 0.8375\n",
      "gelu activation with lecun initializer: 0.6250\n",
      "gelu activation with random initializer: 0.5725\n",
      "gelu activation with uniform initializer: 0.7525\n",
      "\n",
      "Best combination: gelu activation with he initializer, accuracy: 0.8375\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_circles\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# Custom Mish activation function\n",
    "def mish(x):\n",
    "    return x * tf.math.tanh(tf.math.softplus(x))\n",
    "\n",
    "# Generate Concentric Circles dataset\n",
    "X, y = make_circles(n_samples=2000, noise=0.1, factor=0.5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def create_model(activation, hidden_units, initializer):\n",
    "    model = Sequential([layers.Input(shape=(2,))])\n",
    "    \n",
    "    for i, units in enumerate(hidden_units):\n",
    "        model.add(layers.Dense(units, \n",
    "                               kernel_initializer=initializer, \n",
    "                               bias_initializer=initializer, \n",
    "                               name=f'dense_{i}'))\n",
    "        if activation == 'mish':\n",
    "            model.add(layers.Activation(mish))\n",
    "        else:\n",
    "            model.add(layers.Activation(activation))\n",
    "    \n",
    "    model.add(layers.Dense(1, \n",
    "                           activation='sigmoid', \n",
    "                           kernel_initializer=initializer, \n",
    "                           bias_initializer=initializer, \n",
    "                           name='output'))\n",
    "    return model\n",
    "\n",
    "def run_experiment(activation, initializer_name, initializer, dataset_name):\n",
    "    # Model parameters\n",
    "    H = [4, 2]\n",
    "    learning_rate = 0.001\n",
    "    epochs = 20\n",
    "    batch_size = 32\n",
    "\n",
    "    # Create model\n",
    "    model = create_model(activation, H, initializer)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_split=0.2,\n",
    "                        verbose=0)\n",
    "\n",
    "    # Final evaluation\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    num_params = model.count_params()\n",
    "    \n",
    "    print(f\"Activation: {activation}, Initializer: {initializer_name}, Final Test Accuracy: {test_accuracy:.4f}, Num Params: {num_params}\")\n",
    "    \n",
    "    # Save results to CSV\n",
    "    save_results_to_csv(test_accuracy, num_params, activation, initializer_name, dataset_name)\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "def save_results_to_csv(test_accuracy, num_params, activation, weight_init, dataset):\n",
    "    csv_filename = 'experiment_results.csv'\n",
    "    headers = ['test_accuracy', 'num_params', 'activation', 'weight_init', 'dataset']\n",
    "\n",
    "    row = [test_accuracy, num_params, activation, weight_init, dataset]\n",
    "\n",
    "    file_exists = os.path.isfile(csv_filename)\n",
    "    \n",
    "    with open(csv_filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writerow(headers)\n",
    "        \n",
    "        writer.writerow(row)\n",
    "\n",
    "# Define activations and initializers\n",
    "activations = ['relu', 'leaky_relu', 'elu', 'softplus', 'swish', 'mish', 'gelu']\n",
    "initializers = {\n",
    "    'xavier': tf.keras.initializers.GlorotUniform(),\n",
    "    'he': tf.keras.initializers.HeNormal(),\n",
    "    'lecun': tf.keras.initializers.LecunUniform(),\n",
    "    'random': tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05),\n",
    "    'uniform': tf.keras.initializers.RandomUniform(minval=-0.05, maxval=0.05)\n",
    "}\n",
    "\n",
    "# Dataset name\n",
    "dataset_name = 'Concentric Circles'\n",
    "\n",
    "# Run experiments for each activation and initializer\n",
    "results = {}\n",
    "for activation in activations:\n",
    "    for init_name, initializer in initializers.items():\n",
    "        print(f\"\\nRunning experiment with {activation} activation and {init_name} initializer...\")\n",
    "        accuracy = run_experiment(activation, init_name, initializer, dataset_name)\n",
    "        results[(activation, init_name)] = accuracy\n",
    "\n",
    "# Print summary of results\n",
    "print(\"\\nSummary of Results:\")\n",
    "for (activation, init_name), accuracy in results.items():\n",
    "    print(f\"{activation} activation with {init_name} initializer: {accuracy:.4f}\")\n",
    "\n",
    "# Find the best combination\n",
    "best_combo = max(results, key=results.get)\n",
    "print(f\"\\nBest combination: {best_combo[0]} activation with {best_combo[1]} initializer, accuracy: {results[best_combo]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca1c28c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
